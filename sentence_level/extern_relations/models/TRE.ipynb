{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRE_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiPUhgWN3Vhq"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL03i8K26fbr",
        "outputId": "b418f135-1433-4c00-ddaf-b19a6c68c24e"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks' \n",
        "#os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pyg0upH6EOb"
      },
      "source": [
        "data_folder = '/content/drive/MyDrive/Datasets/NREL/task1'\n",
        "with open(os.path.join(data_folder, 'train.json'), 'r')  as f:\n",
        "  train = json.load(f)\n",
        "with open(os.path.join(data_folder, 'dev.json'), 'r')  as f:\n",
        "  dev = json.load(f)\n",
        "with open(os.path.join(data_folder, 'test.json'), 'r')  as f:\n",
        "  test = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_veYkoA-vBm",
        "outputId": "e8b79271-911e-4149-eed2-0be339036896"
      },
      "source": [
        "%pip install -r /content/drive/MyDrive/nn_models/TRE/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.0.1.post2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/24/260f228857c652b58ee6f3ff7cacc9d987cea776a1ece6b76d794dcb7058/torch-1.0.1.post2-cp37-cp37m-manylinux1_x86_64.whl (582.5MB)\n",
            "\u001b[K     |████████████████████████████████| 582.5MB 29kB/s \n",
            "\u001b[?25hCollecting spacy==2.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/29/ede5977ea144bb5758407542eb363ebfb11bbb459d26dea5dd0545563854/spacy-2.1.3-cp37-cp37m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 143kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 4)) (0.0)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.5MB/s \n",
            "\u001b[?25hCollecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 7)) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 8)) (3.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 9)) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 10)) (1.1.5)\n",
            "Collecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 12)) (0.3.3)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.8MB/s \n",
            "\u001b[?25hCollecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 25.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (1.0.5)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (0.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 4)) (0.22.2.post1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 5)) (0.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 10)) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r /content/drive/MyDrive/nn_models/TRE/requirements.txt (line 4)) (1.0.1)\n",
            "Building wheels for collected packages: ftfy, fire, regex\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.1-cp37-none-any.whl size=41573 sha256=bfe5daa1c297b1ee07493b354ea167f91cd08662e067436a1e5a3ab33b011662\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/73/c7/9056e14b04919e5c262fe80b54133b1a88d73683d05d7ac65c\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=bc61ca7529a5f3cc82701865d79ff37b14ac025ff32dbc22d8eeda0bc31be029\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp37-cp37m-linux_x86_64.whl size=534402 sha256=4756c3fe836a505a79d45f8d655192c15406b7000a3384decaac6e68547a5f04\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built ftfy fire regex\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.0.1.post2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, plac, preshed, blis, thinc, spacy, ftfy, fire, regex\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed blis-0.2.4 fire-0.4.0 ftfy-6.0.1 plac-0.9.6 preshed-2.0.1 regex-2017.4.5 spacy-2.1.3 thinc-7.0.8 torch-1.0.1.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdjtocqlArdO",
        "outputId": "dbf53375-df4e-4664-fad5-4d3abc660c5e"
      },
      "source": [
        "!cd /content/drive/MyDrive/nn_models/TRE;\\\n",
        "python dataset_converter.py /content/drive/MyDrive/Datasets/NREL/task1 /content/drive/MyDrive/Datasets/NREL/task1_jsonl --dataset=tacred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(dataset='tacred', dataset_dir='/content/drive/MyDrive/Datasets/NREL/task1', output_dir='/content/drive/MyDrive/Datasets/NREL/task1_jsonl', subsample=False)\n",
            "Converting dataset to jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXxLWO0J6aDb",
        "outputId": "496a9dd3-f512-47ad-df2f-253383b37038"
      },
      "source": [
        "!cd /content/drive/MyDrive/nn_models/TRE;\\\n",
        "CUDA_VISIBLE_DEVICES=0 python relation_extraction.py train \\\n",
        "  --write-model True \\\n",
        "  --masking-mode grammar_and_ner \\\n",
        "  --batch-size 8 \\\n",
        "  --max-epochs 7 \\\n",
        "  --lm-coef 0.5 \\\n",
        "  --learning-rate 1e-5 \\\n",
        "  --learning-rate-warmup 0.002 \\\n",
        "  --clf-pdrop 0.1 \\\n",
        "  --attn-pdrop 0.1 \\\n",
        "  --word-pdrop 0.0 \\\n",
        "  --dataset tacred \\\n",
        "  --data-dir /content/drive/MyDrive/Datasets/NREL/task1/data/jsonl \\\n",
        "  --seed=0 \\\n",
        "  --log-dir /content/drive/MyDrive/Models/TRE_NREL/attempt_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dataset': 'tacred', 'data_dir': '/content/drive/MyDrive/Datasets/NREL/task1/data/jsonl', 'log_dir': '/content/drive/MyDrive/Models/TRE_NREL/attempt_1', 'max_grad_norm': 1, 'learning_rate': 1e-05, 'learning_rate_warmup': 0.002, 'n_ctx': 512, 'n_embd': 768, 'n_head': 12, 'n_layer': 12, 'embd_pdrop': 0.1, 'lm_coef': 0.5, 'attn_pdrop': 0.1, 'resid_pdrop': 0.1, 'clf_pdrop': 0.1, 'word_pdrop': 0.0, 'l2': 0.01, 'vector_l2': True, 'optimizer': 'adam', 'afn': 'gelu', 'learning_rate_schedule': 'warmup_linear', 'encoder_path': 'model/encoder_bpe_40000.json', 'bpe_path': 'model/vocab_40000.bpe', 'n_transfer': 12, 'beta1': 0.9, 'beta2': 0.999, 'e': 1e-08, 'batch_size': 8, 'max_epochs': 7, 'dev_size': 0.1, 'seed': 0, 'load_pre_trained': True, 'subsampling_rate': 1.0, 'train_set_limit': None, 'dev_file': None, 'dev_set_limit': None, 'skip_test_set': False, 'verbose_fetcher': False, 'verbose_training': False, 'masking_mode': 'grammar_and_ner', 'write_model': True}\n",
            "\n",
            "Logging to /content/drive/MyDrive/Models/TRE_NREL/attempt_1/2021-04-23__00-46__692189\n",
            "\n",
            "Device: cuda | n_gpu: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Loading weights...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
            "epoch 1 - iter 0/8907 - loss 6.96973658 - acc 0.00 - micro f1 0.00 - macro f1 0.00\n",
            " 10%|███▊                                  | 890/8907 [09:20<1:24:50,  1.58it/s]epoch 1 - iter 890/8907 - loss 2.85727539 - acc 73.96 - micro f1 0.14 - macro f1 0.02\n",
            " 20%|███████▍                             | 1780/8907 [18:47<1:15:37,  1.57it/s]epoch 1 - iter 1780/8907 - loss 2.46948106 - acc 77.15 - micro f1 0.26 - macro f1 0.06\n",
            " 30%|███████████                          | 2670/8907 [28:14<1:06:32,  1.56it/s]epoch 1 - iter 2670/8907 - loss 2.29631202 - acc 78.43 - micro f1 0.32 - macro f1 0.08\n",
            " 40%|███████████████▌                       | 3560/8907 [37:41<56:40,  1.57it/s]epoch 1 - iter 3560/8907 - loss 2.18318986 - acc 79.21 - micro f1 0.36 - macro f1 0.10\n",
            " 50%|███████████████████▍                   | 4450/8907 [47:08<47:01,  1.58it/s]epoch 1 - iter 4450/8907 - loss 2.09876562 - acc 79.91 - micro f1 0.38 - macro f1 0.11\n",
            " 60%|███████████████████████▍               | 5340/8907 [56:35<38:02,  1.56it/s]epoch 1 - iter 5340/8907 - loss 2.03610926 - acc 80.13 - micro f1 0.40 - macro f1 0.11\n",
            " 70%|█████████████████████████▉           | 6230/8907 [1:06:04<28:22,  1.57it/s]epoch 1 - iter 6230/8907 - loss 1.98266188 - acc 80.53 - micro f1 0.41 - macro f1 0.12\n",
            " 80%|█████████████████████████████▌       | 7120/8907 [1:15:33<18:51,  1.58it/s]epoch 1 - iter 7120/8907 - loss 1.93791185 - acc 80.74 - micro f1 0.43 - macro f1 0.13\n",
            " 90%|█████████████████████████████████▎   | 8010/8907 [1:25:02<09:30,  1.57it/s]epoch 1 - iter 8010/8907 - loss 1.89970666 - acc 81.02 - micro f1 0.44 - macro f1 0.14\n",
            "100%|████████████████████████████████████▉| 8900/8907 [1:34:32<00:04,  1.57it/s]epoch 1 - iter 8900/8907 - loss 1.86250829 - acc 81.34 - micro f1 0.45 - macro f1 0.14\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 4.337040151557638 | acc: 82.21084270288323 | micro f1: 0.5046826222684703 | macro f1: 0.20942749051836398\n",
            "Saving model at epoch 1. With dev_f1 score of 0.20942749051836398.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 2 - iter 0/8907 - loss 1.24537969 - acc 87.50 - micro f1 0.67 - macro f1 0.02\n",
            " 10%|███▊                                  | 890/8907 [09:27<1:25:45,  1.56it/s]epoch 2 - iter 890/8907 - loss 1.50102097 - acc 83.49 - micro f1 0.54 - macro f1 0.19\n",
            " 20%|███████▍                             | 1780/8907 [18:56<1:15:50,  1.57it/s]epoch 2 - iter 1780/8907 - loss 1.49217769 - acc 84.17 - micro f1 0.55 - macro f1 0.20\n",
            " 30%|███████████                          | 2670/8907 [28:24<1:05:53,  1.58it/s]epoch 2 - iter 2670/8907 - loss 1.48447909 - acc 84.14 - micro f1 0.55 - macro f1 0.20\n",
            " 40%|███████████████▌                       | 3560/8907 [37:54<56:22,  1.58it/s]epoch 2 - iter 3560/8907 - loss 1.47395195 - acc 84.32 - micro f1 0.56 - macro f1 0.21\n",
            " 50%|███████████████████▍                   | 4450/8907 [47:22<47:29,  1.56it/s]epoch 2 - iter 4450/8907 - loss 1.46274152 - acc 84.44 - micro f1 0.56 - macro f1 0.21\n",
            " 55%|█████████████████████▍                 | 4885/8907 [52:00<42:44,  1.57it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8qshi7q0RId",
        "outputId": "dbe08a75-e7f0-4225-99a6-f0634d6dc06e"
      },
      "source": [
        "!cd /content/drive/MyDrive/nn_models/TRE;\\\n",
        "CUDA_VISIBLE_DEVICES=0 python relation_extraction.py train \\\n",
        "  --write-model True \\\n",
        "  --masking-mode grammar_and_ner \\\n",
        "  --batch-size 8 \\\n",
        "  --max-epochs 5 \\\n",
        "  --lm-coef 0.5 \\\n",
        "  --learning-rate 1e-4 \\\n",
        "  --learning-rate-warmup 0.002 \\\n",
        "  --clf-pdrop 0.1 \\\n",
        "  --attn-pdrop 0.1 \\\n",
        "  --word-pdrop 0.0 \\\n",
        "  --dataset tacred \\\n",
        "  --data-dir /content/drive/MyDrive/Datasets/NREL/task1/data/jsonl \\\n",
        "  --seed=0 \\\n",
        "  --log-dir /content/drive/MyDrive/Models/TRE_NREL/attempt_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dataset': 'tacred', 'data_dir': '/content/drive/MyDrive/Datasets/NREL/task1/data/jsonl', 'log_dir': '/content/drive/MyDrive/Models/TRE_NREL/attempt_2', 'max_grad_norm': 1, 'learning_rate': 0.0001, 'learning_rate_warmup': 0.002, 'n_ctx': 512, 'n_embd': 768, 'n_head': 12, 'n_layer': 12, 'embd_pdrop': 0.1, 'lm_coef': 0.5, 'attn_pdrop': 0.1, 'resid_pdrop': 0.1, 'clf_pdrop': 0.1, 'word_pdrop': 0.0, 'l2': 0.01, 'vector_l2': True, 'optimizer': 'adam', 'afn': 'gelu', 'learning_rate_schedule': 'warmup_linear', 'encoder_path': 'model/encoder_bpe_40000.json', 'bpe_path': 'model/vocab_40000.bpe', 'n_transfer': 12, 'beta1': 0.9, 'beta2': 0.999, 'e': 1e-08, 'batch_size': 8, 'max_epochs': 5, 'dev_size': 0.1, 'seed': 0, 'load_pre_trained': True, 'subsampling_rate': 1.0, 'train_set_limit': None, 'dev_file': None, 'dev_set_limit': None, 'skip_test_set': False, 'verbose_fetcher': False, 'verbose_training': False, 'masking_mode': 'grammar_and_ner', 'write_model': True}\n",
            "\n",
            "Logging to /content/drive/MyDrive/Models/TRE_NREL/attempt_2/2021-04-23__08-27__206504\n",
            "\n",
            "Device: cuda | n_gpu: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Loading weights...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
            "epoch 1 - iter 0/8907 - loss 7.28644705 - acc 0.00 - micro f1 0.00 - macro f1 0.00\n",
            " 10%|███▉                                    | 890/8907 [02:59<26:54,  4.97it/s]epoch 1 - iter 890/8907 - loss 2.39702042 - acc 75.98 - micro f1 0.17 - macro f1 0.03\n",
            " 20%|███████▊                               | 1780/8907 [05:58<24:01,  4.95it/s]epoch 1 - iter 1780/8907 - loss 2.10286684 - acc 77.89 - micro f1 0.28 - macro f1 0.07\n",
            " 30%|███████████▋                           | 2670/8907 [08:59<20:53,  4.98it/s]epoch 1 - iter 2670/8907 - loss 1.95630685 - acc 78.68 - micro f1 0.33 - macro f1 0.09\n",
            " 40%|███████████████▌                       | 3560/8907 [11:59<17:55,  4.97it/s]epoch 1 - iter 3560/8907 - loss 1.85498784 - acc 79.19 - micro f1 0.36 - macro f1 0.11\n",
            " 50%|███████████████████▍                   | 4450/8907 [15:00<14:57,  4.96it/s]epoch 1 - iter 4450/8907 - loss 1.76857842 - acc 79.80 - micro f1 0.38 - macro f1 0.12\n",
            " 60%|███████████████████████▍               | 5340/8907 [18:01<11:59,  4.96it/s]epoch 1 - iter 5340/8907 - loss 1.70156644 - acc 80.09 - micro f1 0.40 - macro f1 0.13\n",
            " 70%|███████████████████████████▎           | 6230/8907 [21:03<08:56,  4.99it/s]epoch 1 - iter 6230/8907 - loss 1.64466647 - acc 80.55 - micro f1 0.43 - macro f1 0.14\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [24:04<05:58,  4.99it/s]epoch 1 - iter 7120/8907 - loss 1.59515827 - acc 80.81 - micro f1 0.44 - macro f1 0.16\n",
            " 90%|███████████████████████████████████    | 8010/8907 [27:06<03:01,  4.95it/s]epoch 1 - iter 8010/8907 - loss 1.55292440 - acc 81.09 - micro f1 0.45 - macro f1 0.17\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [30:09<00:01,  4.90it/s]epoch 1 - iter 8900/8907 - loss 1.51441700 - acc 81.41 - micro f1 0.47 - macro f1 0.18\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 4.235963968342587 | acc: 82.23310697985083 | micro f1: 0.5098445595854921 | macro f1: 0.24078698573245835\n",
            "Saving model at epoch 1. With dev_f1 score of 0.24078698573245835.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 2 - iter 0/8907 - loss 0.82103330 - acc 100.00 - micro f1 1.00 - macro f1 0.04\n",
            " 10%|███▉                                    | 890/8907 [02:59<26:55,  4.96it/s]epoch 2 - iter 890/8907 - loss 1.13067203 - acc 83.98 - micro f1 0.56 - macro f1 0.24\n",
            " 20%|███████▊                               | 1780/8907 [05:59<24:05,  4.93it/s]epoch 2 - iter 1780/8907 - loss 1.11753736 - acc 84.28 - micro f1 0.56 - macro f1 0.27\n",
            " 30%|███████████▋                           | 2670/8907 [09:00<21:03,  4.94it/s]epoch 2 - iter 2670/8907 - loss 1.10843262 - acc 84.34 - micro f1 0.57 - macro f1 0.29\n",
            " 40%|███████████████▌                       | 3560/8907 [12:02<17:49,  5.00it/s]epoch 2 - iter 3560/8907 - loss 1.09427949 - acc 84.49 - micro f1 0.57 - macro f1 0.29\n",
            " 50%|███████████████████▍                   | 4450/8907 [15:02<15:00,  4.95it/s]epoch 2 - iter 4450/8907 - loss 1.08181723 - acc 84.67 - micro f1 0.58 - macro f1 0.30\n",
            " 60%|███████████████████████▍               | 5340/8907 [18:03<11:57,  4.97it/s]epoch 2 - iter 5340/8907 - loss 1.07002280 - acc 84.78 - micro f1 0.58 - macro f1 0.31\n",
            " 70%|███████████████████████████▎           | 6230/8907 [21:06<09:02,  4.93it/s]epoch 2 - iter 6230/8907 - loss 1.06105006 - acc 84.90 - micro f1 0.59 - macro f1 0.32\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [24:09<06:01,  4.95it/s]epoch 2 - iter 7120/8907 - loss 1.05252816 - acc 84.99 - micro f1 0.59 - macro f1 0.32\n",
            " 90%|███████████████████████████████████    | 8010/8907 [27:12<02:59,  4.99it/s]epoch 2 - iter 8010/8907 - loss 1.04140297 - acc 85.13 - micro f1 0.59 - macro f1 0.32\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [30:14<00:01,  4.96it/s]epoch 2 - iter 8900/8907 - loss 1.03309382 - acc 85.19 - micro f1 0.60 - macro f1 0.33\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 3.8337301823101027 | acc: 85.45029500166982 | micro f1: 0.6201899050474764 | macro f1: 0.33098222436401115\n",
            "Saving model at epoch 2. With dev_f1 score of 0.33098222436401115.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 3 - iter 0/8907 - loss 1.28076172 - acc 87.50 - micro f1 0.00 - macro f1 0.00\n",
            " 10%|███▉                                    | 890/8907 [03:00<27:04,  4.94it/s]epoch 3 - iter 890/8907 - loss 0.89536457 - acc 87.28 - micro f1 0.65 - macro f1 0.38\n",
            " 20%|███████▊                               | 1780/8907 [06:01<24:02,  4.94it/s]epoch 3 - iter 1780/8907 - loss 0.89470356 - acc 87.60 - micro f1 0.66 - macro f1 0.41\n",
            " 30%|███████████▋                           | 2670/8907 [09:02<20:51,  4.98it/s]epoch 3 - iter 2670/8907 - loss 0.89430748 - acc 87.38 - micro f1 0.66 - macro f1 0.40\n",
            " 40%|███████████████▌                       | 3560/8907 [12:01<17:54,  4.97it/s]epoch 3 - iter 3560/8907 - loss 0.88854374 - acc 87.59 - micro f1 0.66 - macro f1 0.40\n",
            " 50%|███████████████████▍                   | 4450/8907 [15:02<15:01,  4.95it/s]epoch 3 - iter 4450/8907 - loss 0.88557627 - acc 87.62 - micro f1 0.66 - macro f1 0.40\n",
            " 60%|███████████████████████▍               | 5340/8907 [18:04<12:02,  4.94it/s]epoch 3 - iter 5340/8907 - loss 0.87991402 - acc 87.70 - micro f1 0.66 - macro f1 0.41\n",
            " 70%|███████████████████████████▎           | 6230/8907 [21:07<08:59,  4.96it/s]epoch 3 - iter 6230/8907 - loss 0.87150700 - acc 87.80 - micro f1 0.67 - macro f1 0.41\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [24:10<05:59,  4.97it/s]epoch 3 - iter 7120/8907 - loss 0.86929685 - acc 87.73 - micro f1 0.66 - macro f1 0.42\n",
            " 90%|███████████████████████████████████    | 8010/8907 [27:13<03:00,  4.96it/s]epoch 3 - iter 8010/8907 - loss 0.86207850 - acc 87.79 - micro f1 0.67 - macro f1 0.43\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [30:17<00:01,  4.92it/s]epoch 3 - iter 8900/8907 - loss 0.85799246 - acc 87.83 - micro f1 0.67 - macro f1 0.43\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 3.598623371304747 | acc: 86.80841589669376 | micro f1: 0.6478946008783261 | macro f1: 0.3719127344884892\n",
            "Saving model at epoch 3. With dev_f1 score of 0.3719127344884892.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 4 - iter 0/8907 - loss 0.44368249 - acc 100.00 - micro f1 1.00 - macro f1 0.04\n",
            " 10%|███▉                                    | 890/8907 [03:00<26:59,  4.95it/s]epoch 4 - iter 890/8907 - loss 0.75435036 - acc 90.10 - micro f1 0.72 - macro f1 0.49\n",
            " 20%|███████▊                               | 1780/8907 [06:01<23:54,  4.97it/s]epoch 4 - iter 1780/8907 - loss 0.74128642 - acc 90.15 - micro f1 0.72 - macro f1 0.49\n",
            " 30%|███████████▋                           | 2670/8907 [09:01<20:51,  4.99it/s]epoch 4 - iter 2670/8907 - loss 0.74244937 - acc 90.18 - micro f1 0.73 - macro f1 0.51\n",
            " 40%|███████████████▌                       | 3560/8907 [12:02<18:02,  4.94it/s]epoch 4 - iter 3560/8907 - loss 0.74131797 - acc 90.10 - micro f1 0.72 - macro f1 0.52\n",
            " 50%|███████████████████▍                   | 4450/8907 [15:03<15:13,  4.88it/s]epoch 4 - iter 4450/8907 - loss 0.73795264 - acc 90.05 - micro f1 0.72 - macro f1 0.52\n",
            " 60%|███████████████████████▍               | 5340/8907 [18:05<11:59,  4.96it/s]epoch 4 - iter 5340/8907 - loss 0.73063726 - acc 90.21 - micro f1 0.73 - macro f1 0.52\n",
            " 70%|███████████████████████████▎           | 6230/8907 [21:06<08:55,  5.00it/s]epoch 4 - iter 6230/8907 - loss 0.72665608 - acc 90.30 - micro f1 0.73 - macro f1 0.54\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [24:07<05:59,  4.97it/s]epoch 4 - iter 7120/8907 - loss 0.72268840 - acc 90.29 - micro f1 0.73 - macro f1 0.54\n",
            " 90%|███████████████████████████████████    | 8010/8907 [27:09<03:00,  4.98it/s]epoch 4 - iter 8010/8907 - loss 0.71926726 - acc 90.36 - micro f1 0.73 - macro f1 0.54\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [30:12<00:01,  4.92it/s]epoch 4 - iter 8900/8907 - loss 0.71581537 - acc 90.38 - micro f1 0.73 - macro f1 0.54\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 3.6132394783642567 | acc: 87.20917288211065 | micro f1: 0.6632073140597974 | macro f1: 0.41216049830342844\n",
            "Saving model at epoch 4. With dev_f1 score of 0.41216049830342844.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 5 - iter 0/8907 - loss 0.35062462 - acc 100.00 - micro f1 1.00 - macro f1 0.02\n",
            " 10%|███▉                                    | 890/8907 [02:59<26:43,  5.00it/s]epoch 5 - iter 890/8907 - loss 0.57344252 - acc 93.49 - micro f1 0.81 - macro f1 0.59\n",
            " 20%|███████▊                               | 1780/8907 [05:58<23:52,  4.98it/s]epoch 5 - iter 1780/8907 - loss 0.58431641 - acc 93.35 - micro f1 0.81 - macro f1 0.62\n",
            " 30%|███████████▋                           | 2670/8907 [08:58<20:49,  4.99it/s]epoch 5 - iter 2670/8907 - loss 0.57835756 - acc 93.55 - micro f1 0.81 - macro f1 0.64\n",
            " 40%|███████████████▌                       | 3560/8907 [11:59<18:01,  4.94it/s]epoch 5 - iter 3560/8907 - loss 0.58368182 - acc 93.34 - micro f1 0.81 - macro f1 0.64\n",
            " 50%|███████████████████▍                   | 4450/8907 [15:00<15:02,  4.94it/s]epoch 5 - iter 4450/8907 - loss 0.58527735 - acc 93.36 - micro f1 0.81 - macro f1 0.64\n",
            " 60%|███████████████████████▍               | 5340/8907 [18:02<12:05,  4.92it/s]epoch 5 - iter 5340/8907 - loss 0.58327352 - acc 93.36 - micro f1 0.81 - macro f1 0.64\n",
            " 70%|███████████████████████████▎           | 6230/8907 [21:03<09:00,  4.95it/s]epoch 5 - iter 6230/8907 - loss 0.58021990 - acc 93.40 - micro f1 0.81 - macro f1 0.65\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [24:06<06:02,  4.93it/s]epoch 5 - iter 7120/8907 - loss 0.57838318 - acc 93.40 - micro f1 0.81 - macro f1 0.65\n",
            " 90%|███████████████████████████████████    | 8010/8907 [27:08<03:00,  4.96it/s]epoch 5 - iter 8010/8907 - loss 0.57632475 - acc 93.45 - micro f1 0.81 - macro f1 0.65\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [30:11<00:01,  4.98it/s]epoch 5 - iter 8900/8907 - loss 0.57403074 - acc 93.45 - micro f1 0.81 - macro f1 0.65\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 4.395543690315933 | acc: 87.05332294333742 | micro f1: 0.6657210401891254 | macro f1: 0.4502522635296071\n",
            "Saving model at epoch 5. With dev_f1 score of 0.4502522635296071.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHHHL9p_uTQO",
        "outputId": "f7251bc1-639a-484d-eed9-0c2aed27ea78"
      },
      "source": [
        "!cd /content/drive/MyDrive/nn_models/TRE;\\\n",
        "CUDA_VISIBLE_DEVICES=0 python relation_extraction.py train \\\n",
        "  --write-model True \\\n",
        "  --masking-mode grammar_and_ner \\\n",
        "  --batch-size 8 \\\n",
        "  --max-epochs 8 \\\n",
        "  --lm-coef 0.5 \\\n",
        "  --learning-rate 1e-4 \\\n",
        "  --learning-rate-warmup 0.002 \\\n",
        "  --clf-pdrop 0.1 \\\n",
        "  --attn-pdrop 0.1 \\\n",
        "  --word-pdrop 0.0 \\\n",
        "  --dataset tacred \\\n",
        "  --data-dir /content/drive/MyDrive/Datasets/NREL/task1/data/jsonl \\\n",
        "  --seed=0 \\\n",
        "  --log-dir /content/drive/MyDrive/Models/TRE_NREL/attempt_3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dataset': 'tacred', 'data_dir': '/content/drive/MyDrive/Datasets/NREL/task1/data/jsonl', 'log_dir': '/content/drive/MyDrive/Models/TRE_NREL/attempt_3', 'max_grad_norm': 1, 'learning_rate': 0.0001, 'learning_rate_warmup': 0.002, 'n_ctx': 512, 'n_embd': 768, 'n_head': 12, 'n_layer': 12, 'embd_pdrop': 0.1, 'lm_coef': 0.5, 'attn_pdrop': 0.1, 'resid_pdrop': 0.1, 'clf_pdrop': 0.1, 'word_pdrop': 0.0, 'l2': 0.01, 'vector_l2': True, 'optimizer': 'adam', 'afn': 'gelu', 'learning_rate_schedule': 'warmup_linear', 'encoder_path': 'model/encoder_bpe_40000.json', 'bpe_path': 'model/vocab_40000.bpe', 'n_transfer': 12, 'beta1': 0.9, 'beta2': 0.999, 'e': 1e-08, 'batch_size': 8, 'max_epochs': 8, 'dev_size': 0.1, 'seed': 0, 'load_pre_trained': True, 'subsampling_rate': 1.0, 'train_set_limit': None, 'dev_file': None, 'dev_set_limit': None, 'skip_test_set': False, 'verbose_fetcher': False, 'verbose_training': False, 'masking_mode': 'grammar_and_ner', 'write_model': True}\n",
            "\n",
            "Logging to /content/drive/MyDrive/Models/TRE_NREL/attempt_3/2021-04-23__12-57__192671\n",
            "\n",
            "Device: cuda | n_gpu: 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "Loading weights...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
            "epoch 1 - iter 0/8907 - loss 7.45401621 - acc 0.00 - micro f1 0.00 - macro f1 0.00\n",
            " 10%|███▉                                    | 890/8907 [05:02<45:15,  2.95it/s]epoch 1 - iter 890/8907 - loss 2.50484441 - acc 75.51 - micro f1 0.13 - macro f1 0.02\n",
            " 20%|███████▊                               | 1780/8907 [10:04<40:11,  2.95it/s]epoch 1 - iter 1780/8907 - loss 2.19773442 - acc 77.32 - micro f1 0.23 - macro f1 0.04\n",
            " 30%|███████████▋                           | 2670/8907 [15:06<35:08,  2.96it/s]epoch 1 - iter 2670/8907 - loss 2.03271724 - acc 78.22 - micro f1 0.29 - macro f1 0.07\n",
            " 40%|███████████████▌                       | 3560/8907 [20:09<30:12,  2.95it/s]epoch 1 - iter 3560/8907 - loss 1.92011317 - acc 78.60 - micro f1 0.31 - macro f1 0.08\n",
            " 50%|███████████████████▍                   | 4450/8907 [25:12<25:07,  2.96it/s]epoch 1 - iter 4450/8907 - loss 1.83102438 - acc 79.13 - micro f1 0.33 - macro f1 0.09\n",
            " 60%|███████████████████████▍               | 5340/8907 [30:15<19:58,  2.98it/s]epoch 1 - iter 5340/8907 - loss 1.76481267 - acc 79.41 - micro f1 0.35 - macro f1 0.11\n",
            " 70%|███████████████████████████▎           | 6230/8907 [35:19<15:03,  2.96it/s]epoch 1 - iter 6230/8907 - loss 1.70261584 - acc 79.95 - micro f1 0.38 - macro f1 0.12\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [40:23<10:08,  2.93it/s]epoch 1 - iter 7120/8907 - loss 1.65041835 - acc 80.20 - micro f1 0.40 - macro f1 0.13\n",
            " 90%|███████████████████████████████████    | 8010/8907 [45:28<05:03,  2.95it/s]epoch 1 - iter 8010/8907 - loss 1.60592555 - acc 80.50 - micro f1 0.42 - macro f1 0.14\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [50:33<00:02,  2.95it/s]epoch 1 - iter 8900/8907 - loss 1.56507180 - acc 80.85 - micro f1 0.43 - macro f1 0.15\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 4.274553080251644 | acc: 82.901035288879 | micro f1: 0.5284531124766231 | macro f1: 0.22184607579489904\n",
            "Saving model at epoch 1. With dev_f1 score of 0.22184607579489904.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 2 - iter 0/8907 - loss 0.90532959 - acc 75.00 - micro f1 0.00 - macro f1 0.00\n",
            " 10%|███▉                                    | 890/8907 [05:02<45:13,  2.95it/s]epoch 2 - iter 890/8907 - loss 1.16063792 - acc 83.87 - micro f1 0.56 - macro f1 0.23\n",
            " 20%|███████▊                               | 1780/8907 [10:04<40:24,  2.94it/s]epoch 2 - iter 1780/8907 - loss 1.14433712 - acc 84.40 - micro f1 0.57 - macro f1 0.24\n",
            " 30%|███████████▋                           | 2670/8907 [15:07<35:22,  2.94it/s]epoch 2 - iter 2670/8907 - loss 1.14087055 - acc 84.45 - micro f1 0.57 - macro f1 0.25\n",
            " 40%|███████████████▌                       | 3560/8907 [20:10<30:12,  2.95it/s]epoch 2 - iter 3560/8907 - loss 1.12951523 - acc 84.58 - micro f1 0.57 - macro f1 0.26\n",
            " 50%|███████████████████▍                   | 4450/8907 [25:13<25:10,  2.95it/s]epoch 2 - iter 4450/8907 - loss 1.11888380 - acc 84.72 - micro f1 0.58 - macro f1 0.26\n",
            " 60%|███████████████████████▍               | 5340/8907 [30:17<20:13,  2.94it/s]epoch 2 - iter 5340/8907 - loss 1.10883825 - acc 84.75 - micro f1 0.58 - macro f1 0.27\n",
            " 70%|███████████████████████████▎           | 6230/8907 [35:21<15:06,  2.95it/s]epoch 2 - iter 6230/8907 - loss 1.09920284 - acc 84.81 - micro f1 0.58 - macro f1 0.27\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [40:26<10:06,  2.95it/s]epoch 2 - iter 7120/8907 - loss 1.09103405 - acc 84.85 - micro f1 0.58 - macro f1 0.28\n",
            " 90%|███████████████████████████████████    | 8010/8907 [45:30<05:04,  2.95it/s]epoch 2 - iter 8010/8907 - loss 1.08091948 - acc 84.97 - micro f1 0.59 - macro f1 0.28\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [50:35<00:02,  2.95it/s]epoch 2 - iter 8900/8907 - loss 1.07244082 - acc 85.00 - micro f1 0.59 - macro f1 0.29\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 4.300020548066622 | acc: 84.77123455415784 | micro f1: 0.5895620246525046 | macro f1: 0.2591240250803594\n",
            "Saving model at epoch 2. With dev_f1 score of 0.2591240250803594.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 3 - iter 0/8907 - loss 1.20902371 - acc 87.50 - micro f1 0.00 - macro f1 0.00\n",
            " 10%|███▉                                    | 890/8907 [05:02<45:30,  2.94it/s]epoch 3 - iter 890/8907 - loss 0.94121739 - acc 86.63 - micro f1 0.62 - macro f1 0.32\n",
            " 20%|███████▊                               | 1780/8907 [10:04<40:22,  2.94it/s]epoch 3 - iter 1780/8907 - loss 0.94012255 - acc 86.85 - micro f1 0.64 - macro f1 0.36\n",
            " 30%|███████████▋                           | 2670/8907 [15:07<35:14,  2.95it/s]epoch 3 - iter 2670/8907 - loss 0.94018887 - acc 86.82 - micro f1 0.64 - macro f1 0.38\n",
            " 40%|███████████████▌                       | 3560/8907 [20:10<30:12,  2.95it/s]epoch 3 - iter 3560/8907 - loss 0.93371773 - acc 86.96 - micro f1 0.65 - macro f1 0.38\n",
            " 50%|███████████████████▍                   | 4450/8907 [25:14<25:09,  2.95it/s]epoch 3 - iter 4450/8907 - loss 0.93025208 - acc 86.95 - micro f1 0.64 - macro f1 0.38\n",
            " 60%|███████████████████████▍               | 5340/8907 [30:18<20:26,  2.91it/s]epoch 3 - iter 5340/8907 - loss 0.92384227 - acc 87.02 - micro f1 0.64 - macro f1 0.38\n",
            " 70%|███████████████████████████▎           | 6230/8907 [35:23<15:06,  2.95it/s]epoch 3 - iter 6230/8907 - loss 0.91743684 - acc 87.07 - micro f1 0.65 - macro f1 0.39\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [40:27<10:04,  2.95it/s]epoch 3 - iter 7120/8907 - loss 0.91607764 - acc 87.01 - micro f1 0.64 - macro f1 0.39\n",
            " 90%|███████████████████████████████████    | 8010/8907 [45:32<05:02,  2.97it/s]epoch 3 - iter 8010/8907 - loss 0.90927291 - acc 87.06 - micro f1 0.64 - macro f1 0.39\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [50:36<00:02,  2.95it/s]epoch 3 - iter 8900/8907 - loss 0.90496022 - acc 87.05 - micro f1 0.64 - macro f1 0.39\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 3.5008083361766125 | acc: 86.71935878882333 | micro f1: 0.6416184971098265 | macro f1: 0.35659876641211924\n",
            "Saving model at epoch 3. With dev_f1 score of 0.35659876641211924.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 4 - iter 0/8907 - loss 0.46967968 - acc 100.00 - micro f1 1.00 - macro f1 0.04\n",
            " 10%|███▉                                    | 890/8907 [05:02<45:31,  2.94it/s]epoch 4 - iter 890/8907 - loss 0.81336795 - acc 88.96 - micro f1 0.69 - macro f1 0.42\n",
            " 20%|███████▊                               | 1780/8907 [10:05<40:13,  2.95it/s]epoch 4 - iter 1780/8907 - loss 0.79888221 - acc 89.16 - micro f1 0.70 - macro f1 0.44\n",
            " 30%|███████████▋                           | 2670/8907 [15:07<35:14,  2.95it/s]epoch 4 - iter 2670/8907 - loss 0.80375923 - acc 88.87 - micro f1 0.69 - macro f1 0.45\n",
            " 40%|███████████████▌                       | 3560/8907 [20:10<30:16,  2.94it/s]epoch 4 - iter 3560/8907 - loss 0.80486080 - acc 88.71 - micro f1 0.69 - macro f1 0.46\n",
            " 50%|███████████████████▍                   | 4450/8907 [25:13<25:10,  2.95it/s]epoch 4 - iter 4450/8907 - loss 0.80295549 - acc 88.66 - micro f1 0.69 - macro f1 0.46\n",
            " 60%|███████████████████████▍               | 5340/8907 [30:17<20:11,  2.94it/s]epoch 4 - iter 5340/8907 - loss 0.79485470 - acc 88.88 - micro f1 0.70 - macro f1 0.47\n",
            " 70%|███████████████████████████▎           | 6230/8907 [35:20<15:06,  2.95it/s]epoch 4 - iter 6230/8907 - loss 0.79070561 - acc 88.97 - micro f1 0.70 - macro f1 0.48\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [40:23<10:02,  2.97it/s]epoch 4 - iter 7120/8907 - loss 0.78822862 - acc 88.94 - micro f1 0.70 - macro f1 0.48\n",
            " 90%|███████████████████████████████████    | 8010/8907 [45:28<05:03,  2.95it/s]epoch 4 - iter 8010/8907 - loss 0.78511906 - acc 88.98 - micro f1 0.70 - macro f1 0.48\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [50:32<00:02,  2.95it/s]epoch 4 - iter 8900/8907 - loss 0.78227992 - acc 89.01 - micro f1 0.70 - macro f1 0.49\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 3.696926821449957 | acc: 87.07558722030501 | micro f1: 0.6537678207739307 | macro f1: 0.4121182903049696\n",
            "Saving model at epoch 4. With dev_f1 score of 0.4121182903049696.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 5 - iter 0/8907 - loss 0.58944046 - acc 87.50 - micro f1 0.00 - macro f1 0.00\n",
            " 10%|███▉                                    | 890/8907 [05:01<45:12,  2.96it/s]epoch 5 - iter 890/8907 - loss 0.65277475 - acc 91.86 - micro f1 0.77 - macro f1 0.54\n",
            " 20%|███████▊                               | 1780/8907 [10:04<40:08,  2.96it/s]epoch 5 - iter 1780/8907 - loss 0.66717768 - acc 91.38 - micro f1 0.76 - macro f1 0.55\n",
            " 30%|███████████▋                           | 2670/8907 [15:06<35:17,  2.95it/s]epoch 5 - iter 2670/8907 - loss 0.66028273 - acc 91.56 - micro f1 0.76 - macro f1 0.57\n",
            " 40%|███████████████▌                       | 3560/8907 [20:09<30:16,  2.94it/s]epoch 5 - iter 3560/8907 - loss 0.66720205 - acc 91.34 - micro f1 0.76 - macro f1 0.57\n",
            " 50%|███████████████████▍                   | 4450/8907 [25:12<25:20,  2.93it/s]epoch 5 - iter 4450/8907 - loss 0.66699545 - acc 91.39 - micro f1 0.76 - macro f1 0.57\n",
            " 60%|███████████████████████▍               | 5340/8907 [30:16<20:11,  2.94it/s]epoch 5 - iter 5340/8907 - loss 0.66633234 - acc 91.36 - micro f1 0.76 - macro f1 0.57\n",
            " 70%|███████████████████████████▎           | 6230/8907 [35:21<15:04,  2.96it/s]epoch 5 - iter 6230/8907 - loss 0.66474428 - acc 91.35 - micro f1 0.76 - macro f1 0.57\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [40:26<10:05,  2.95it/s]epoch 5 - iter 7120/8907 - loss 0.66650171 - acc 91.25 - micro f1 0.76 - macro f1 0.57\n",
            " 90%|███████████████████████████████████    | 8010/8907 [45:31<05:05,  2.93it/s]epoch 5 - iter 8010/8907 - loss 0.66484153 - acc 91.29 - micro f1 0.76 - macro f1 0.57\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [50:38<00:02,  2.94it/s]epoch 5 - iter 8900/8907 - loss 0.66248467 - acc 91.30 - micro f1 0.76 - macro f1 0.57\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 4.445752418507025 | acc: 87.03105866636982 | micro f1: 0.6576819407008087 | macro f1: 0.4255945246866085\n",
            "Saving model at epoch 5. With dev_f1 score of 0.4255945246866085.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 6 - iter 0/8907 - loss 0.64668632 - acc 87.50 - micro f1 0.00 - macro f1 0.00\n",
            " 10%|███▉                                    | 890/8907 [05:03<45:21,  2.95it/s]epoch 6 - iter 890/8907 - loss 0.53262275 - acc 94.04 - micro f1 0.83 - macro f1 0.62\n",
            " 20%|███████▊                               | 1780/8907 [10:07<40:48,  2.91it/s]epoch 6 - iter 1780/8907 - loss 0.53884714 - acc 93.90 - micro f1 0.83 - macro f1 0.64\n",
            " 30%|███████████▋                           | 2670/8907 [15:10<35:22,  2.94it/s]epoch 6 - iter 2670/8907 - loss 0.53585803 - acc 94.00 - micro f1 0.83 - macro f1 0.64\n",
            " 40%|███████████████▌                       | 3560/8907 [20:14<30:18,  2.94it/s]epoch 6 - iter 3560/8907 - loss 0.54219376 - acc 93.82 - micro f1 0.83 - macro f1 0.65\n",
            " 50%|███████████████████▍                   | 4450/8907 [25:19<25:12,  2.95it/s]epoch 6 - iter 4450/8907 - loss 0.54404417 - acc 93.74 - micro f1 0.83 - macro f1 0.66\n",
            " 60%|███████████████████████▍               | 5340/8907 [30:24<20:07,  2.95it/s]epoch 6 - iter 5340/8907 - loss 0.54364818 - acc 93.80 - micro f1 0.83 - macro f1 0.66\n",
            " 70%|███████████████████████████▎           | 6230/8907 [35:29<15:07,  2.95it/s]epoch 6 - iter 6230/8907 - loss 0.53955629 - acc 93.89 - micro f1 0.83 - macro f1 0.67\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [40:34<10:08,  2.94it/s]epoch 6 - iter 7120/8907 - loss 0.53856534 - acc 93.88 - micro f1 0.83 - macro f1 0.67\n",
            " 90%|███████████████████████████████████    | 8010/8907 [45:40<05:04,  2.95it/s]epoch 6 - iter 8010/8907 - loss 0.53858963 - acc 93.84 - micro f1 0.83 - macro f1 0.67\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [50:46<00:02,  2.95it/s]epoch 6 - iter 8900/8907 - loss 0.53692747 - acc 93.84 - micro f1 0.83 - macro f1 0.67\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 5.178396232194761 | acc: 86.53011243459868 | micro f1: 0.6597653554175293 | macro f1: 0.4710628573727383\n",
            "Saving model at epoch 6. With dev_f1 score of 0.4710628573727383.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 7 - iter 0/8907 - loss 0.18343943 - acc 100.00 - micro f1 1.00 - macro f1 0.02\n",
            " 10%|███▉                                    | 890/8907 [05:02<45:19,  2.95it/s]epoch 7 - iter 890/8907 - loss 0.43737523 - acc 95.72 - micro f1 0.89 - macro f1 0.76\n",
            " 20%|███████▊                               | 1780/8907 [10:05<40:18,  2.95it/s]epoch 7 - iter 1780/8907 - loss 0.43015354 - acc 95.85 - micro f1 0.89 - macro f1 0.77\n",
            " 30%|███████████▋                           | 2670/8907 [15:07<35:22,  2.94it/s]epoch 7 - iter 2670/8907 - loss 0.42915067 - acc 95.85 - micro f1 0.89 - macro f1 0.76\n",
            " 40%|███████████████▌                       | 3560/8907 [20:10<30:19,  2.94it/s]epoch 7 - iter 3560/8907 - loss 0.42307229 - acc 95.99 - micro f1 0.89 - macro f1 0.76\n",
            " 50%|███████████████████▍                   | 4450/8907 [25:13<25:22,  2.93it/s]epoch 7 - iter 4450/8907 - loss 0.42517881 - acc 95.96 - micro f1 0.89 - macro f1 0.76\n",
            " 60%|███████████████████████▍               | 5340/8907 [30:17<20:16,  2.93it/s]epoch 7 - iter 5340/8907 - loss 0.42236484 - acc 96.03 - micro f1 0.89 - macro f1 0.76\n",
            " 70%|███████████████████████████▎           | 6230/8907 [35:21<15:07,  2.95it/s]epoch 7 - iter 6230/8907 - loss 0.42300437 - acc 96.01 - micro f1 0.89 - macro f1 0.76\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [40:25<10:06,  2.95it/s]epoch 7 - iter 7120/8907 - loss 0.42085121 - acc 96.03 - micro f1 0.89 - macro f1 0.77\n",
            " 90%|███████████████████████████████████    | 8010/8907 [45:30<05:03,  2.95it/s]epoch 7 - iter 8010/8907 - loss 0.41840154 - acc 96.06 - micro f1 0.89 - macro f1 0.77\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [50:35<00:02,  2.95it/s]epoch 7 - iter 8900/8907 - loss 0.41753027 - acc 96.04 - micro f1 0.89 - macro f1 0.77\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 6.309502492380424 | acc: 87.18690860514305 | micro f1: 0.6763194768799626 | macro f1: 0.4791432203874701\n",
            "Saving model at epoch 7. With dev_f1 score of 0.4791432203874701.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "  0%|                                                  | 0/8907 [00:00<?, ?it/s]epoch 8 - iter 0/8907 - loss 0.22379088 - acc 100.00 - micro f1 1.00 - macro f1 0.04\n",
            " 10%|███▉                                    | 890/8907 [05:02<45:26,  2.94it/s]epoch 8 - iter 890/8907 - loss 0.33197895 - acc 97.53 - micro f1 0.93 - macro f1 0.81\n",
            " 20%|███████▊                               | 1780/8907 [10:04<40:02,  2.97it/s]epoch 8 - iter 1780/8907 - loss 0.32916897 - acc 97.57 - micro f1 0.93 - macro f1 0.82\n",
            " 30%|███████████▋                           | 2670/8907 [15:05<35:12,  2.95it/s]epoch 8 - iter 2670/8907 - loss 0.32859088 - acc 97.59 - micro f1 0.93 - macro f1 0.83\n",
            " 40%|███████████████▌                       | 3560/8907 [20:07<30:07,  2.96it/s]epoch 8 - iter 3560/8907 - loss 0.32715932 - acc 97.65 - micro f1 0.93 - macro f1 0.83\n",
            " 50%|███████████████████▍                   | 4450/8907 [25:09<25:12,  2.95it/s]epoch 8 - iter 4450/8907 - loss 0.32688023 - acc 97.65 - micro f1 0.93 - macro f1 0.84\n",
            " 60%|███████████████████████▍               | 5340/8907 [30:11<20:13,  2.94it/s]epoch 8 - iter 5340/8907 - loss 0.32809149 - acc 97.64 - micro f1 0.93 - macro f1 0.84\n",
            " 70%|███████████████████████████▎           | 6230/8907 [35:14<15:06,  2.95it/s]epoch 8 - iter 6230/8907 - loss 0.32851641 - acc 97.60 - micro f1 0.93 - macro f1 0.84\n",
            " 80%|███████████████████████████████▏       | 7120/8907 [40:17<10:02,  2.96it/s]epoch 8 - iter 7120/8907 - loss 0.32813031 - acc 97.61 - micro f1 0.93 - macro f1 0.84\n",
            " 90%|███████████████████████████████████    | 8010/8907 [45:21<05:04,  2.94it/s]epoch 8 - iter 8010/8907 - loss 0.32754073 - acc 97.61 - micro f1 0.93 - macro f1 0.83\n",
            "100%|██████████████████████████████████████▉| 8900/8907 [50:25<00:02,  2.96it/s]epoch 8 - iter 8900/8907 - loss 0.32664841 - acc 97.64 - micro f1 0.93 - macro f1 0.84\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EVALUATION: cost: 7.3914407141739735 | acc: 86.98653011243461 | micro f1: 0.672354161863039 | macro f1: 0.4921571518249629\n",
            "Saving model at epoch 8. With dev_f1 score of 0.4921571518249629.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3b82yGL7Qbw"
      },
      "source": [
        "sys.path.insert(0,'/content/drive/MyDrive/nn_models/TRE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yLlJSweDRlx",
        "outputId": "32a1b411-c718-42a0-8bc3-08671e27a914"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 8.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp37-none-any.whl size=11074434 sha256=b604d5579601d651201f4486a85ef7106160eb477a8c73c5bf8582c488a875a8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wyq871z3/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34SZr6MPEjlB",
        "outputId": "a9d4c539-bbca-4e2d-dea0-cd043e8572aa"
      },
      "source": [
        "!cd /content/drive/MyDrive/nn_models/TRE;/bin/bash /content/drive/MyDrive/nn_models/TRE/download-model.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading to model\n",
            "/content/drive/MyDrive/nn_models/TRE/model /content/drive/MyDrive/nn_models/TRE\n",
            "--2021-04-23 12:53:49--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/encoder_bpe_40000.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815973 (797K) [text/plain]\n",
            "Saving to: ‘encoder_bpe_40000.json.3’\n",
            "\n",
            "encoder_bpe_40000.j 100%[===================>] 796.85K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-04-23 12:53:50 (20.2 MB/s) - ‘encoder_bpe_40000.json.3’ saved [815973/815973]\n",
            "\n",
            "--2021-04-23 12:53:50--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_0.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614044 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_0.npy.3’\n",
            "\n",
            "params_0.npy.3      100%[===================>]  44.45M  57.2MB/s    in 0.8s    \n",
            "\n",
            "2021-04-23 12:53:52 (57.2 MB/s) - ‘params_0.npy.3’ saved [46614044/46614044]\n",
            "\n",
            "--2021-04-23 12:53:52--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_1.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614044 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_1.npy.3’\n",
            "\n",
            "params_1.npy.3      100%[===================>]  44.45M  50.9MB/s    in 0.9s    \n",
            "\n",
            "2021-04-23 12:53:54 (50.9 MB/s) - ‘params_1.npy.3’ saved [46614044/46614044]\n",
            "\n",
            "--2021-04-23 12:53:54--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_2.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614044 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_2.npy.3’\n",
            "\n",
            "params_2.npy.3      100%[===================>]  44.45M  54.3MB/s    in 0.8s    \n",
            "\n",
            "2021-04-23 12:53:57 (54.3 MB/s) - ‘params_2.npy.3’ saved [46614044/46614044]\n",
            "\n",
            "--2021-04-23 12:53:57--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_3.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614044 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_3.npy.3’\n",
            "\n",
            "params_3.npy.3      100%[===================>]  44.45M  56.0MB/s    in 0.8s    \n",
            "\n",
            "2021-04-23 12:53:59 (56.0 MB/s) - ‘params_3.npy.3’ saved [46614044/46614044]\n",
            "\n",
            "--2021-04-23 12:53:59--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_4.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614040 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_4.npy.3’\n",
            "\n",
            "params_4.npy.3      100%[===================>]  44.45M  54.8MB/s    in 0.8s    \n",
            "\n",
            "2021-04-23 12:54:02 (54.8 MB/s) - ‘params_4.npy.3’ saved [46614040/46614040]\n",
            "\n",
            "--2021-04-23 12:54:02--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_5.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614040 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_5.npy.3’\n",
            "\n",
            "params_5.npy.3      100%[===================>]  44.45M  46.2MB/s    in 1.0s    \n",
            "\n",
            "2021-04-23 12:54:05 (46.2 MB/s) - ‘params_5.npy.3’ saved [46614040/46614040]\n",
            "\n",
            "--2021-04-23 12:54:05--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_6.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614040 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_6.npy.3’\n",
            "\n",
            "params_6.npy.3      100%[===================>]  44.45M  55.9MB/s    in 0.8s    \n",
            "\n",
            "2021-04-23 12:54:07 (55.9 MB/s) - ‘params_6.npy.3’ saved [46614040/46614040]\n",
            "\n",
            "--2021-04-23 12:54:08--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_7.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614040 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_7.npy.3’\n",
            "\n",
            "params_7.npy.3      100%[===================>]  44.45M  57.6MB/s    in 0.8s    \n",
            "\n",
            "2021-04-23 12:54:11 (57.6 MB/s) - ‘params_7.npy.3’ saved [46614040/46614040]\n",
            "\n",
            "--2021-04-23 12:54:11--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_8.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614040 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_8.npy.3’\n",
            "\n",
            "params_8.npy.3      100%[===================>]  44.45M  55.3MB/s    in 0.8s    \n",
            "\n",
            "2021-04-23 12:54:13 (55.3 MB/s) - ‘params_8.npy.3’ saved [46614040/46614040]\n",
            "\n",
            "--2021-04-23 12:54:13--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_9.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46614040 (44M) [application/octet-stream]\n",
            "Saving to: ‘params_9.npy.3’\n",
            "\n",
            "params_9.npy.3      100%[===================>]  44.45M  43.1MB/s    in 1.0s    \n",
            "\n",
            "2021-04-23 12:54:16 (43.1 MB/s) - ‘params_9.npy.3’ saved [46614040/46614040]\n",
            "\n",
            "--2021-04-23 12:54:16--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/params_shapes.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1478 (1.4K) [text/plain]\n",
            "Saving to: ‘params_shapes.json.3’\n",
            "\n",
            "params_shapes.json. 100%[===================>]   1.44K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-04-23 12:54:17 (1.30 MB/s) - ‘params_shapes.json.3’ saved [1478/1478]\n",
            "\n",
            "--2021-04-23 12:54:17--  https://raw.githubusercontent.com/openai/finetune-transformer-lm/master/model/vocab_40000.bpe\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 458495 (448K) [text/plain]\n",
            "Saving to: ‘vocab_40000.bpe.3’\n",
            "\n",
            "vocab_40000.bpe.3   100%[===================>] 447.75K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-04-23 12:54:17 (11.5 MB/s) - ‘vocab_40000.bpe.3’ saved [458495/458495]\n",
            "\n",
            "/content/drive/MyDrive/nn_models/TRE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtf4FmIhEncz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOAaxTX0d-Rx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzixO3H0d-vc"
      },
      "source": [
        "Посмотрим BPE словарь\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzllzubpeCCI"
      },
      "source": [
        "tre_path = '/content/drive/MyDrive/nn_models/TRE/model'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s99e1KJzeLXG"
      },
      "source": [
        "with open(tre_path + '/encoder_bpe_40000.json', 'r') as f:\n",
        "  bpe_vocab0 = json.load(f)\n",
        "with open(tre_path + '/encoder_bpe_40000.json.2', 'r') as f:\n",
        "  bpe_vocab2 = json.load(f)\n",
        "with open(tre_path + '/params_shapes.json', 'r') as f:\n",
        "  params = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvrx9kHgePgj",
        "outputId": "99b7b741-6fc0-477c-e6ae-faaf705b24e2"
      },
      "source": [
        "!ls /content/drive/MyDrive/nn_models/TRE/model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_bpe_40000.json\t  params_2.npy.1  params_5.npy.2  params_9.npy\n",
            "encoder_bpe_40000.json.1  params_2.npy.2  params_6.npy\t  params_9.npy.1\n",
            "encoder_bpe_40000.json.2  params_3.npy\t  params_6.npy.1  params_9.npy.2\n",
            "params_0.npy\t\t  params_3.npy.1  params_6.npy.2  params_shapes.json\n",
            "params_0.npy.1\t\t  params_3.npy.2  params_7.npy\t  params_shapes.json.1\n",
            "params_0.npy.2\t\t  params_4.npy\t  params_7.npy.1  params_shapes.json.2\n",
            "params_1.npy\t\t  params_4.npy.1  params_7.npy.2  vocab_40000.bpe\n",
            "params_1.npy.1\t\t  params_4.npy.2  params_8.npy\t  vocab_40000.bpe.1\n",
            "params_1.npy.2\t\t  params_5.npy\t  params_8.npy.1  vocab_40000.bpe.2\n",
            "params_2.npy\t\t  params_5.npy.1  params_8.npy.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrvKNo8kebBi",
        "outputId": "5ed607d2-9f77-4103-a7b2-ea821214028f"
      },
      "source": [
        "type(bpe_vocab0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOGyd0s-e49h",
        "outputId": "177a3f92-c7e6-4f29-fd0b-bdf79b35edfc"
      },
      "source": [
        "list(bpe_vocab0.items())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 1),\n",
              " (',', 2),\n",
              " ('t', 3),\n",
              " ('h', 4),\n",
              " ('e', 5),\n",
              " ('\"', 6),\n",
              " ('o', 7),\n",
              " ('a', 8),\n",
              " ('n', 9),\n",
              " ('d', 10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aad54jOflZ-",
        "outputId": "f9c2b60e-5f44-4dfa-c8bf-f8f1d77cf8bb"
      },
      "source": [
        "len(bpe_vocab0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40478"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGk-HQQ2fo9h",
        "outputId": "6f7859a6-2007-41e7-ec7c-6464acf416ef"
      },
      "source": [
        "len(bpe_vocab2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40478"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6uqZMlifrFL",
        "outputId": "32ef90fc-26d3-459f-e8bf-9bb6bfa3ed3d"
      },
      "source": [
        "list(bpe_vocab0.keys())[:1000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.',\n",
              " ',',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " '\"',\n",
              " 'o',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " 'i',\n",
              " 'f',\n",
              " 'w',\n",
              " 's',\n",
              " 'y',\n",
              " 'u',\n",
              " 'r',\n",
              " \"'\",\n",
              " '?',\n",
              " 'm',\n",
              " 'b',\n",
              " '-',\n",
              " 'v',\n",
              " 'p',\n",
              " 'c',\n",
              " 'l',\n",
              " 'k',\n",
              " 'j',\n",
              " '!',\n",
              " 'g',\n",
              " '*',\n",
              " ';',\n",
              " ':',\n",
              " 'x',\n",
              " 'q',\n",
              " 'z',\n",
              " ')',\n",
              " '(',\n",
              " '1',\n",
              " '/',\n",
              " '_',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '~',\n",
              " '5',\n",
              " '#',\n",
              " '0',\n",
              " '6',\n",
              " '7',\n",
              " '$',\n",
              " '>',\n",
              " '9',\n",
              " '8',\n",
              " '[',\n",
              " ']',\n",
              " '<',\n",
              " '&',\n",
              " '%',\n",
              " '¨',\n",
              " '`',\n",
              " 'é',\n",
              " '»',\n",
              " '«',\n",
              " '=',\n",
              " '•',\n",
              " '@',\n",
              " '+',\n",
              " '©',\n",
              " '¡',\n",
              " '{',\n",
              " '}',\n",
              " 'ª',\n",
              " 'ñ',\n",
              " 'ï',\n",
              " '‖',\n",
              " 'ç',\n",
              " 'í',\n",
              " '^',\n",
              " '£',\n",
              " '§',\n",
              " '♥',\n",
              " '−',\n",
              " 'à',\n",
              " '|',\n",
              " '°',\n",
              " '¦',\n",
              " 'ł',\n",
              " 'ĩ',\n",
              " 'ü',\n",
              " '®',\n",
              " 'ù',\n",
              " 'á',\n",
              " 'â',\n",
              " 'ó',\n",
              " 'è',\n",
              " '∞',\n",
              " 'ë',\n",
              " 'ä',\n",
              " '♪',\n",
              " 'ò',\n",
              " 'ω',\n",
              " '▪',\n",
              " '½',\n",
              " 'ǒ',\n",
              " '‡',\n",
              " 'ê',\n",
              " '◊',\n",
              " '►',\n",
              " '۞',\n",
              " 'ú',\n",
              " '€',\n",
              " 'æ',\n",
              " 'î',\n",
              " '↕',\n",
              " 'ô',\n",
              " 'ē',\n",
              " 'ǐ',\n",
              " '♫',\n",
              " '�',\n",
              " '\\uf04a',\n",
              " '™',\n",
              " 'ř',\n",
              " 'ā',\n",
              " '·',\n",
              " '¿',\n",
              " '\\\\',\n",
              " '─',\n",
              " '\\uf067',\n",
              " '\\uf020',\n",
              " '∙',\n",
              " 'ॐ',\n",
              " 'ö',\n",
              " 'ø',\n",
              " '\\uf06c',\n",
              " '\\uf09b',\n",
              " '●',\n",
              " '\\xad',\n",
              " '■',\n",
              " '\\uf063',\n",
              " '†',\n",
              " 'å',\n",
              " 'ō',\n",
              " 'ã',\n",
              " '¤',\n",
              " '⠔',\n",
              " '\\uf059',\n",
              " 'н',\n",
              " '\\uf09a',\n",
              " '⚔',\n",
              " 'ă',\n",
              " 'û',\n",
              " 'º',\n",
              " '♦',\n",
              " 'ĝ',\n",
              " '¹',\n",
              " '═',\n",
              " '\\uf0a3',\n",
              " '¾',\n",
              " 'ì',\n",
              " '☼',\n",
              " 'ș',\n",
              " '¼',\n",
              " '☺',\n",
              " 'đ',\n",
              " 'ą',\n",
              " 'ǽ',\n",
              " '╦',\n",
              " '\\uf02a',\n",
              " '¬',\n",
              " 'ī',\n",
              " '\\u200b',\n",
              " 'œ',\n",
              " '¢',\n",
              " 'ǎ',\n",
              " 'š',\n",
              " 'ʻ',\n",
              " 'ν',\n",
              " 'α',\n",
              " '\\uf05d',\n",
              " 'я',\n",
              " 'б',\n",
              " 'й',\n",
              " 'τ',\n",
              " 'ο',\n",
              " 'ε',\n",
              " 'ί',\n",
              " 'ι',\n",
              " 'δ',\n",
              " '\\uf073',\n",
              " '\\uf05e',\n",
              " '‐',\n",
              " 'с',\n",
              " '\\uf0a5',\n",
              " 'þ',\n",
              " 'κ',\n",
              " '‑',\n",
              " '‒',\n",
              " 'к',\n",
              " '\\uf07e',\n",
              " 'σ',\n",
              " 'č',\n",
              " '\\uf0be',\n",
              " 'υ',\n",
              " 'ό',\n",
              " '\\uf061',\n",
              " '\\uf02d',\n",
              " 'ß',\n",
              " 'е',\n",
              " 'т',\n",
              " 'μ',\n",
              " 'π',\n",
              " 'ρ',\n",
              " 'έ',\n",
              " 'в',\n",
              " 'ş',\n",
              " 'о',\n",
              " 'ð',\n",
              " 'а',\n",
              " 'ς',\n",
              " 'р',\n",
              " 'м',\n",
              " 'у',\n",
              " 'ή',\n",
              " 'ά',\n",
              " 'и',\n",
              " 'д',\n",
              " '\\uf0bc',\n",
              " '\\uf070',\n",
              " 'λ',\n",
              " 'л',\n",
              " 'γ',\n",
              " '¯',\n",
              " '\\uf065',\n",
              " '\\uf043',\n",
              " '\\uf074',\n",
              " '\\uf068',\n",
              " '\\uf072',\n",
              " '.</w>',\n",
              " ',</w>',\n",
              " 't</w>',\n",
              " 'h</w>',\n",
              " 'e</w>',\n",
              " '\"</w>',\n",
              " 'o</w>',\n",
              " 'a</w>',\n",
              " 'n</w>',\n",
              " 'd</w>',\n",
              " 'i</w>',\n",
              " 'f</w>',\n",
              " 'w</w>',\n",
              " 's</w>',\n",
              " 'y</w>',\n",
              " 'u</w>',\n",
              " 'r</w>',\n",
              " \"'</w>\",\n",
              " '?</w>',\n",
              " 'm</w>',\n",
              " 'b</w>',\n",
              " '-</w>',\n",
              " 'v</w>',\n",
              " 'p</w>',\n",
              " 'c</w>',\n",
              " 'l</w>',\n",
              " 'k</w>',\n",
              " 'j</w>',\n",
              " '!</w>',\n",
              " 'g</w>',\n",
              " '*</w>',\n",
              " ';</w>',\n",
              " ':</w>',\n",
              " 'x</w>',\n",
              " 'q</w>',\n",
              " 'z</w>',\n",
              " ')</w>',\n",
              " '(</w>',\n",
              " '1</w>',\n",
              " '/</w>',\n",
              " '_</w>',\n",
              " '2</w>',\n",
              " '3</w>',\n",
              " '4</w>',\n",
              " '~</w>',\n",
              " '5</w>',\n",
              " '#</w>',\n",
              " '0</w>',\n",
              " '6</w>',\n",
              " '7</w>',\n",
              " '$</w>',\n",
              " '></w>',\n",
              " '9</w>',\n",
              " '8</w>',\n",
              " '[</w>',\n",
              " ']</w>',\n",
              " '<</w>',\n",
              " '&</w>',\n",
              " '%</w>',\n",
              " '¨</w>',\n",
              " '`</w>',\n",
              " 'é</w>',\n",
              " '»</w>',\n",
              " '«</w>',\n",
              " '=</w>',\n",
              " '•</w>',\n",
              " '@</w>',\n",
              " '+</w>',\n",
              " '©</w>',\n",
              " '¡</w>',\n",
              " '{</w>',\n",
              " '}</w>',\n",
              " 'ª</w>',\n",
              " 'ñ</w>',\n",
              " 'ï</w>',\n",
              " '‖</w>',\n",
              " 'ç</w>',\n",
              " 'í</w>',\n",
              " '^</w>',\n",
              " '£</w>',\n",
              " '§</w>',\n",
              " '♥</w>',\n",
              " '−</w>',\n",
              " 'à</w>',\n",
              " '|</w>',\n",
              " '°</w>',\n",
              " '¦</w>',\n",
              " 'ł</w>',\n",
              " 'ĩ</w>',\n",
              " 'ü</w>',\n",
              " '®</w>',\n",
              " 'ù</w>',\n",
              " 'á</w>',\n",
              " 'â</w>',\n",
              " 'ó</w>',\n",
              " 'è</w>',\n",
              " '∞</w>',\n",
              " 'ë</w>',\n",
              " 'ä</w>',\n",
              " '♪</w>',\n",
              " 'ò</w>',\n",
              " 'ω</w>',\n",
              " '▪</w>',\n",
              " '½</w>',\n",
              " 'ǒ</w>',\n",
              " '‡</w>',\n",
              " 'ê</w>',\n",
              " '◊</w>',\n",
              " '►</w>',\n",
              " '۞</w>',\n",
              " 'ú</w>',\n",
              " '€</w>',\n",
              " 'æ</w>',\n",
              " 'î</w>',\n",
              " '↕</w>',\n",
              " 'ô</w>',\n",
              " 'ē</w>',\n",
              " 'ǐ</w>',\n",
              " '♫</w>',\n",
              " '�</w>',\n",
              " '\\uf04a</w>',\n",
              " '™</w>',\n",
              " 'ř</w>',\n",
              " 'ā</w>',\n",
              " '·</w>',\n",
              " '¿</w>',\n",
              " '\\\\</w>',\n",
              " '─</w>',\n",
              " '\\uf067</w>',\n",
              " '\\uf020</w>',\n",
              " '∙</w>',\n",
              " 'ॐ</w>',\n",
              " 'ö</w>',\n",
              " 'ø</w>',\n",
              " '\\uf06c</w>',\n",
              " '\\uf09b</w>',\n",
              " '●</w>',\n",
              " '\\xad</w>',\n",
              " '■</w>',\n",
              " '\\uf063</w>',\n",
              " '†</w>',\n",
              " 'å</w>',\n",
              " 'ō</w>',\n",
              " 'ã</w>',\n",
              " '¤</w>',\n",
              " '⠔</w>',\n",
              " '\\uf059</w>',\n",
              " 'н</w>',\n",
              " '\\uf09a</w>',\n",
              " '⚔</w>',\n",
              " 'ă</w>',\n",
              " 'û</w>',\n",
              " 'º</w>',\n",
              " '♦</w>',\n",
              " 'ĝ</w>',\n",
              " '¹</w>',\n",
              " '═</w>',\n",
              " '\\uf0a3</w>',\n",
              " '¾</w>',\n",
              " 'ì</w>',\n",
              " '☼</w>',\n",
              " 'ș</w>',\n",
              " '¼</w>',\n",
              " '☺</w>',\n",
              " 'đ</w>',\n",
              " 'ą</w>',\n",
              " 'ǽ</w>',\n",
              " '╦</w>',\n",
              " '\\uf02a</w>',\n",
              " '¬</w>',\n",
              " 'ī</w>',\n",
              " '\\u200b</w>',\n",
              " 'œ</w>',\n",
              " '¢</w>',\n",
              " 'ǎ</w>',\n",
              " 'š</w>',\n",
              " 'ʻ</w>',\n",
              " 'ν</w>',\n",
              " 'α</w>',\n",
              " '\\uf05d</w>',\n",
              " 'я</w>',\n",
              " 'б</w>',\n",
              " 'й</w>',\n",
              " 'τ</w>',\n",
              " 'ο</w>',\n",
              " 'ε</w>',\n",
              " 'ί</w>',\n",
              " 'ι</w>',\n",
              " 'δ</w>',\n",
              " '\\uf073</w>',\n",
              " '\\uf05e</w>',\n",
              " '‐</w>',\n",
              " 'с</w>',\n",
              " '\\uf0a5</w>',\n",
              " 'þ</w>',\n",
              " 'κ</w>',\n",
              " '‑</w>',\n",
              " '‒</w>',\n",
              " 'к</w>',\n",
              " '\\uf07e</w>',\n",
              " 'σ</w>',\n",
              " 'č</w>',\n",
              " '\\uf0be</w>',\n",
              " 'υ</w>',\n",
              " 'ό</w>',\n",
              " '\\uf061</w>',\n",
              " '\\uf02d</w>',\n",
              " 'ß</w>',\n",
              " 'е</w>',\n",
              " 'т</w>',\n",
              " 'μ</w>',\n",
              " 'π</w>',\n",
              " 'ρ</w>',\n",
              " 'έ</w>',\n",
              " 'в</w>',\n",
              " 'ş</w>',\n",
              " 'о</w>',\n",
              " 'ð</w>',\n",
              " 'а</w>',\n",
              " 'ς</w>',\n",
              " 'р</w>',\n",
              " 'м</w>',\n",
              " 'у</w>',\n",
              " 'ή</w>',\n",
              " 'ά</w>',\n",
              " 'и</w>',\n",
              " 'д</w>',\n",
              " '\\uf0bc</w>',\n",
              " '\\uf070</w>',\n",
              " 'λ</w>',\n",
              " 'л</w>',\n",
              " 'γ</w>',\n",
              " '¯</w>',\n",
              " '\\uf065</w>',\n",
              " '\\uf043</w>',\n",
              " '\\uf074</w>',\n",
              " '\\uf068</w>',\n",
              " '\\uf072</w>',\n",
              " 'th',\n",
              " 'in',\n",
              " 'ed</w>',\n",
              " 'an',\n",
              " 'the</w>',\n",
              " 'ou',\n",
              " 'er</w>',\n",
              " 'ing</w>',\n",
              " 'to</w>',\n",
              " 'er',\n",
              " 'he</w>',\n",
              " 'and</w>',\n",
              " 'ar',\n",
              " 'hi',\n",
              " 'at</w>',\n",
              " 're',\n",
              " 'wa',\n",
              " 'on',\n",
              " 'st',\n",
              " 'en',\n",
              " 'ha',\n",
              " 'of</w>',\n",
              " 'or',\n",
              " 'in</w>',\n",
              " 'al',\n",
              " 'it',\n",
              " 'en</w>',\n",
              " 'on</w>',\n",
              " 'el',\n",
              " 'ro',\n",
              " 'it</w>',\n",
              " 'ac',\n",
              " 'was</w>',\n",
              " 'me</w>',\n",
              " 'yo',\n",
              " 'you</w>',\n",
              " 'her</w>',\n",
              " 'es</w>',\n",
              " 'ly</w>',\n",
              " 'no',\n",
              " 'at',\n",
              " 'lo',\n",
              " 'li',\n",
              " 'she</w>',\n",
              " 'wh',\n",
              " 'or</w>',\n",
              " 'st</w>',\n",
              " 'his</w>',\n",
              " 'that</w>',\n",
              " 'ea',\n",
              " 've</w>',\n",
              " 'be',\n",
              " 'ri',\n",
              " 'ld</w>',\n",
              " 'an</w>',\n",
              " 'gh',\n",
              " 'ere</w>',\n",
              " 'the',\n",
              " \"'s</w>\",\n",
              " 'ti',\n",
              " \"'t</w>\",\n",
              " \"n't</w>\",\n",
              " 'id</w>',\n",
              " 'sa',\n",
              " 'le</w>',\n",
              " 'si',\n",
              " 'ur',\n",
              " 'is</w>',\n",
              " 'bu',\n",
              " 'se</w>',\n",
              " 'my</w>',\n",
              " 'ho',\n",
              " 'ould</w>',\n",
              " 'ne',\n",
              " 'out</w>',\n",
              " 'le',\n",
              " 'wit',\n",
              " 'om',\n",
              " 'il',\n",
              " 'with</w>',\n",
              " 'as</w>',\n",
              " 'had</w>',\n",
              " 'se',\n",
              " 'ght</w>',\n",
              " 'ke</w>',\n",
              " 'for</w>',\n",
              " 'un',\n",
              " 'la',\n",
              " 'ra',\n",
              " 'one</w>',\n",
              " 'ma',\n",
              " 'but</w>',\n",
              " 'do',\n",
              " 'ab',\n",
              " 'to',\n",
              " 'ic',\n",
              " 'ch',\n",
              " 'ev',\n",
              " 'him</w>',\n",
              " 'sh',\n",
              " 'ked</w>',\n",
              " 'ca',\n",
              " 'pp',\n",
              " 'be</w>',\n",
              " 'go',\n",
              " 'sp',\n",
              " 'oun',\n",
              " 'ir',\n",
              " 'de',\n",
              " 'ther</w>',\n",
              " 'do</w>',\n",
              " 'co',\n",
              " 'all</w>',\n",
              " 'et</w>',\n",
              " 'ss</w>',\n",
              " 'di',\n",
              " 'mo',\n",
              " 'ent</w>',\n",
              " 'not</w>',\n",
              " 'de</w>',\n",
              " 'now</w>',\n",
              " 'ted</w>',\n",
              " 'what</w>',\n",
              " 'they</w>',\n",
              " 'ag',\n",
              " 'ack</w>',\n",
              " 'said</w>',\n",
              " 'have</w>',\n",
              " 'fro',\n",
              " 'we</w>',\n",
              " 'ch</w>',\n",
              " 'ce</w>',\n",
              " 'up</w>',\n",
              " 'ore</w>',\n",
              " 'bo',\n",
              " 'ver</w>',\n",
              " 'ter</w>',\n",
              " 'loo',\n",
              " 'thing</w>',\n",
              " 'this</w>',\n",
              " 'from</w>',\n",
              " 'king</w>',\n",
              " 'ds</w>',\n",
              " 'so</w>',\n",
              " 'as',\n",
              " 'our</w>',\n",
              " 'su',\n",
              " 'wn</w>',\n",
              " 'con',\n",
              " 'did</w>',\n",
              " 'mi',\n",
              " 'ru',\n",
              " 'fe',\n",
              " 'sed</w>',\n",
              " 'gh</w>',\n",
              " 'ta',\n",
              " 'ju',\n",
              " 'led</w>',\n",
              " 'could</w>',\n",
              " 'would</w>',\n",
              " 'so',\n",
              " 'way</w>',\n",
              " 'ts</w>',\n",
              " 'are</w>',\n",
              " 'were</w>',\n",
              " 'ir</w>',\n",
              " 'da',\n",
              " 'po',\n",
              " 'if</w>',\n",
              " 'em',\n",
              " 'ill</w>',\n",
              " 'rea',\n",
              " 'like</w>',\n",
              " 'ers</w>',\n",
              " 'back</w>',\n",
              " 'wor',\n",
              " 'ear',\n",
              " 'ound</w>',\n",
              " 'there</w>',\n",
              " \"'d</w>\",\n",
              " 'ded</w>',\n",
              " 'ell</w>',\n",
              " 'ex',\n",
              " 'qu',\n",
              " 'ough</w>',\n",
              " 'hea',\n",
              " 'th</w>',\n",
              " 'no</w>',\n",
              " 'll</w>',\n",
              " 'into</w>',\n",
              " 'ing',\n",
              " 'just</w>',\n",
              " 'when</w>',\n",
              " 'about</w>',\n",
              " 'ati',\n",
              " 'fa',\n",
              " 'pu',\n",
              " 'then</w>',\n",
              " 'ally</w>',\n",
              " 'sc',\n",
              " 'lea',\n",
              " 'ver',\n",
              " 'al</w>',\n",
              " 'mu',\n",
              " 'ant</w>',\n",
              " 'ace</w>',\n",
              " 'fu',\n",
              " 'whi',\n",
              " 'yes</w>',\n",
              " 'ind</w>',\n",
              " 'ting</w>',\n",
              " 'them</w>',\n",
              " 'dy</w>',\n",
              " 'com',\n",
              " 'ding</w>',\n",
              " 'gu',\n",
              " 'tur',\n",
              " 'been</w>',\n",
              " 'ee',\n",
              " 'for',\n",
              " 'som',\n",
              " 'ard</w>',\n",
              " 'know</w>',\n",
              " 'some',\n",
              " 'op',\n",
              " 'by</w>',\n",
              " 'tw',\n",
              " 'your</w>',\n",
              " 'ter',\n",
              " 'pro',\n",
              " 'sel',\n",
              " 'of',\n",
              " 'ge</w>',\n",
              " 'fi',\n",
              " 'od</w>',\n",
              " 'pa',\n",
              " 'ec',\n",
              " 'down</w>',\n",
              " 'over</w>',\n",
              " 're</w>',\n",
              " 'lu',\n",
              " 'how</w>',\n",
              " \"'m</w>\",\n",
              " 'time</w>',\n",
              " 'aga',\n",
              " 'wi',\n",
              " 'tr',\n",
              " 'sur',\n",
              " 'more</w>',\n",
              " '..',\n",
              " 'get</w>',\n",
              " 'other</w>',\n",
              " 'pre',\n",
              " 'ned</w>',\n",
              " 'ong</w>',\n",
              " 'der</w>',\n",
              " 'vi',\n",
              " 'par',\n",
              " 'ys</w>',\n",
              " 'pl',\n",
              " 'side</w>',\n",
              " 'fo',\n",
              " 'tly</w>',\n",
              " 'ck</w>',\n",
              " 'eyes</w>',\n",
              " 'ks</w>',\n",
              " 'gi',\n",
              " 'me',\n",
              " 'ine</w>',\n",
              " 'ate</w>',\n",
              " 'ni',\n",
              " 'self</w>',\n",
              " '...</w>',\n",
              " 'per',\n",
              " 'ty</w>',\n",
              " 'af',\n",
              " 'el</w>',\n",
              " 'their</w>',\n",
              " 'ice</w>',\n",
              " 'head</w>',\n",
              " 'thin',\n",
              " 'pped</w>',\n",
              " 'can</w>',\n",
              " 'gr',\n",
              " \"'re</w>\",\n",
              " 'man</w>',\n",
              " 'who</w>',\n",
              " 'ying</w>',\n",
              " 'ling</w>',\n",
              " 'ation</w>',\n",
              " 'sto',\n",
              " 'us</w>',\n",
              " 'sm',\n",
              " 'right</w>',\n",
              " 'der',\n",
              " 'sho',\n",
              " 'ok</w>',\n",
              " 'ge',\n",
              " 'any</w>',\n",
              " 'ga',\n",
              " 'fore</w>',\n",
              " 'pe',\n",
              " 'ever',\n",
              " 'ought</w>',\n",
              " 'before</w>',\n",
              " 'han',\n",
              " 'new</w>',\n",
              " 'even</w>',\n",
              " 'around</w>',\n",
              " 'ely</w>',\n",
              " 'mp',\n",
              " 'see</w>',\n",
              " 'star',\n",
              " 'cau',\n",
              " 'any',\n",
              " 'ved</w>',\n",
              " 'here</w>',\n",
              " 'ss',\n",
              " 'sh</w>',\n",
              " 'clo',\n",
              " 'going</w>',\n",
              " 'fir',\n",
              " 'go</w>',\n",
              " 'our',\n",
              " 'thr',\n",
              " 'ps</w>',\n",
              " 'some</w>',\n",
              " \"'ll</w>\",\n",
              " 'low',\n",
              " 'where</w>',\n",
              " 'ving</w>',\n",
              " 'only</w>',\n",
              " 'tion</w>',\n",
              " 'hel',\n",
              " 'off</w>',\n",
              " 'will</w>',\n",
              " 'na',\n",
              " 'ci',\n",
              " 'than</w>',\n",
              " 'looked</w>',\n",
              " 'able</w>',\n",
              " 'tle</w>',\n",
              " 'roo',\n",
              " 'ons</w>',\n",
              " 'ten',\n",
              " 'through</w>',\n",
              " 'want</w>',\n",
              " 'ous</w>',\n",
              " 'think</w>',\n",
              " 'ning</w>',\n",
              " 'cu',\n",
              " 'hand</w>',\n",
              " 'ba',\n",
              " 'vo',\n",
              " 'mar',\n",
              " 'jo',\n",
              " 'again</w>',\n",
              " 'too</w>',\n",
              " 'face</w>',\n",
              " 'te</w>',\n",
              " 'wal',\n",
              " 'shi',\n",
              " 'sw',\n",
              " 'lit',\n",
              " 'away</w>',\n",
              " 'ft</w>',\n",
              " 'still</w>',\n",
              " 'room</w>',\n",
              " 'ity</w>',\n",
              " 'something</w>',\n",
              " 'fe</w>',\n",
              " 'come</w>',\n",
              " 'ssi',\n",
              " 'day</w>',\n",
              " 'let</w>',\n",
              " 'ry</w>',\n",
              " 'ear</w>',\n",
              " 'ep',\n",
              " 'ings</w>',\n",
              " 'gre',\n",
              " 'car',\n",
              " 'ered</w>',\n",
              " 'est',\n",
              " 'wan',\n",
              " 'after</w>',\n",
              " 'well</w>',\n",
              " 'hear',\n",
              " 'asked</w>',\n",
              " 'bl',\n",
              " 'thought</w>',\n",
              " 'two</w>',\n",
              " 'never</w>',\n",
              " 'ang',\n",
              " 'good</w>',\n",
              " 'ever</w>',\n",
              " 'end</w>',\n",
              " 'sta',\n",
              " 'ad',\n",
              " 'ated</w>',\n",
              " 'br',\n",
              " 'ance</w>',\n",
              " 'min',\n",
              " 'cha',\n",
              " \"'ve</w>\",\n",
              " 'sure</w>',\n",
              " 'ck',\n",
              " 'cause</w>',\n",
              " 'hu',\n",
              " 'made</w>',\n",
              " 'got</w>',\n",
              " 'tri',\n",
              " 'ssed</w>',\n",
              " 'much</w>',\n",
              " 'look</w>',\n",
              " 'ched</w>',\n",
              " 'mb',\n",
              " 'shed</w>',\n",
              " 'fin',\n",
              " 'why</w>',\n",
              " 'du',\n",
              " 'ward</w>',\n",
              " 'bel',\n",
              " 'turned</w>',\n",
              " 'sha',\n",
              " 'gg',\n",
              " 'ach</w>',\n",
              " 'bro',\n",
              " 'gra',\n",
              " 'most</w>',\n",
              " 'knew</w>',\n",
              " 'ath</w>',\n",
              " 'door</w>',\n",
              " 'little</w>',\n",
              " 'tal',\n",
              " 'ls</w>',\n",
              " 'because</w>',\n",
              " 'fel',\n",
              " 'ened</w>',\n",
              " 'tu',\n",
              " 'war',\n",
              " 'te',\n",
              " 'sk',\n",
              " 'ff',\n",
              " 'sit',\n",
              " 'take</w>',\n",
              " 'happ',\n",
              " 'man',\n",
              " 'ms</w>',\n",
              " 'make</w>',\n",
              " 'cal',\n",
              " 'every',\n",
              " 'long</w>',\n",
              " 'first</w>',\n",
              " 'tra',\n",
              " 'ach',\n",
              " 'ste',\n",
              " 'ful</w>',\n",
              " 'ble</w>',\n",
              " 'ess</w>',\n",
              " 'im',\n",
              " 'say</w>',\n",
              " 'ence</w>',\n",
              " 'came</w>',\n",
              " 'ced</w>',\n",
              " 'pri',\n",
              " 'felt</w>',\n",
              " 'bed</w>',\n",
              " 'ree</w>',\n",
              " 'son</w>',\n",
              " 'mon',\n",
              " 'dar',\n",
              " 'took</w>',\n",
              " 'ser',\n",
              " 'app',\n",
              " 'ki',\n",
              " 'tru',\n",
              " 'fri',\n",
              " 'low</w>',\n",
              " 'chi',\n",
              " 'body</w>',\n",
              " 'fr',\n",
              " 'pla',\n",
              " 'sin',\n",
              " 'ali',\n",
              " 'wanted</w>',\n",
              " 'ose</w>',\n",
              " 'very</w>',\n",
              " 'ves</w>',\n",
              " 'est</w>',\n",
              " 'need</w>',\n",
              " 'pul',\n",
              " 'kno',\n",
              " 'ears</w>',\n",
              " 'dd',\n",
              " 'stu',\n",
              " 'tell</w>',\n",
              " 'pi',\n",
              " 'str',\n",
              " 'dre',\n",
              " 'really</w>',\n",
              " 'cre',\n",
              " 'red</w>',\n",
              " 'bi',\n",
              " 'has</w>',\n",
              " 'cont',\n",
              " 'he',\n",
              " 'hands</w>',\n",
              " 'which</w>',\n",
              " 'sen',\n",
              " 'peop',\n",
              " 'its</w>',\n",
              " 'sing</w>',\n",
              " 'people</w>',\n",
              " 'rec',\n",
              " 'wat',\n",
              " 'sli',\n",
              " 'ca</w>',\n",
              " 'should</w>',\n",
              " 'night</w>',\n",
              " 'ws</w>',\n",
              " 'with',\n",
              " 'though</w>',\n",
              " 'left</w>',\n",
              " 'while</w>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipmhYpF5fvxi",
        "outputId": "d448c168-bfd3-4ab4-f5ca-46082660c006"
      },
      "source": [
        "bpe_vocab0 == bpe_vocab2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C5LvHpOf7GG"
      },
      "source": [
        "rus_bpe = [text for text in bpe_vocab0.keys() if re.match(r'[а-яА-ЯёЁ]', text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTyULE47g5Zx",
        "outputId": "bd25f1f2-4b2f-4cf5-e9d9-3c27a0512480"
      },
      "source": [
        "len(rus_bpe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHLbIZdEg-rz",
        "outputId": "30457192-0035-4249-ed13-b3c61c8178a7"
      },
      "source": [
        "rus_bpe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['н',\n",
              " 'я',\n",
              " 'б',\n",
              " 'й',\n",
              " 'с',\n",
              " 'к',\n",
              " 'е',\n",
              " 'т',\n",
              " 'в',\n",
              " 'о',\n",
              " 'а',\n",
              " 'р',\n",
              " 'м',\n",
              " 'у',\n",
              " 'и',\n",
              " 'д',\n",
              " 'л',\n",
              " 'н</w>',\n",
              " 'я</w>',\n",
              " 'б</w>',\n",
              " 'й</w>',\n",
              " 'с</w>',\n",
              " 'к</w>',\n",
              " 'е</w>',\n",
              " 'т</w>',\n",
              " 'в</w>',\n",
              " 'о</w>',\n",
              " 'а</w>',\n",
              " 'р</w>',\n",
              " 'м</w>',\n",
              " 'у</w>',\n",
              " 'и</w>',\n",
              " 'д</w>',\n",
              " 'л</w>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPLriNK9g_5R",
        "outputId": "a67c3ed8-cfbe-4ba5-e4bd-6933bb7e0480"
      },
      "source": [
        "len(params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "146"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPirzw8GmChv",
        "outputId": "429e3592-de85-4a19-ccc7-61b9bf950504"
      },
      "source": [
        "params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[512, 768],\n",
              " [40478, 768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 2304],\n",
              " [2304],\n",
              " [1, 768, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768],\n",
              " [1, 768, 3072],\n",
              " [3072],\n",
              " [1, 3072, 768],\n",
              " [768],\n",
              " [768],\n",
              " [768]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKawu66xmD8F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}